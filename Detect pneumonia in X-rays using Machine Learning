#Connect to Google Drive
from google.colab import drive
drive.mount('/content/gdrive')

#Unzip given data that i have downloaded in my gdrive
!unzip gdrive/My\ Drive/detect-pneumonia-spring-2022.zip

#Import libraries
from keras.models import Sequential
from keras_preprocessing.image import ImageDataGenerator
from keras.layers import Dense, Activation, Flatten, Dropout, BatchNormalization
from keras.layers import Conv2D, MaxPooling2D, MaxPool2D, ZeroPadding2D, AveragePooling2D
from keras import regularizers, optimizers
from keras.callbacks import ReduceLROnPlateau, ModelCheckpoint, Early Stopping
from tensorflow.keras.optimizers import Adam
from sklearn.utils.class_weight import compute_class_weight
from matplotlib import pyplot as plt
import pandas as pd
import numpy as np
import tensorflow as tf
import glob
import cv2
from PIL import Image

#Import labels csv
traindf=pd.read_csv('/content/labels_train.csv',dtype=str)

#Data preprocess and creation of validation set
datagen=ImageDataGenerator(rotation_range=20,
                          shear_range=0.1,
                          fill_mode='nearest',
                          horizontal_flip=True,
                          preprocessing_function=tf.keras.applications.efficientnet_v2.preprocess_input,
                          rescale=1./255.,
                          validation_split=0.20)

#Import training set and validation set
train_generator = datagen.flow_from_dataframe(dataframe=traindf,
                                        directory="/content/train_images",
                                        x_col='file_name',
                                        y_col="class_id",
                                        subset='training',
                                        batch_size=16,
                                        class_mode='categorical',
                                        target_size=(224,224),)
valid_generator = datagen.flow_from_dataframe(dataframe=traindf,
                                      directory="/content/train_images",
                                      x_col='file_name',
                                      y_col="class_id",
                                      subset='validation',
                                      batch_size=16,
                                      class_mode='categorical',
                                      target_size=(224,224))

#Training model
inputs = tf.keras.layers.Input(shape = (224, 224,3))
layer1 = tf.keras.layers.Conv2D(42,(3,3),activation = 'relu',use_bias = 1,kernel_regularizer = 'l2',padding = 'same')(inputs)
MaxPool1 = tf.keras.layers.MaxPool2D(pool_size=(3, 3), strides=(1,1),padding = 'same')(layer1)
layer2 = tf.keras.layers.Conv2D(42,(3,3),activation = 'relu',use_bias = 1,kernel_regularizer= 'l2',padding = 'same')(MaxPool1)
MaxPool2 = tf.keras.layers.MaxPool2D(pool_size=(3, 3), strides=(1,1),padding = 'same')(layer2)

#Inception Layer 1
Inception_layer1_con_1 = tf.keras.layers.Conv2D(42,(3,3),activation = 'relu',use_bias = 1,kernel_regularizer = 'l2',strides = (1,1),padding ='same')(MaxPool2)
Inception_layer1_con_2 = tf.keras.layers.Conv2D(42,(5,5),activation = 'relu',use_bias = 1,kernel_regularizer = 'l2',strides = (1,1),padding ='same')(MaxPool2)
MaxPool3 = tf.keras.layers.MaxPool2D(pool_size=(3, 3), strides=(1,1),padding = 'same')(MaxPool2)
Concatenate1 = tf.keras.layers.Concatenate(axis=-1)([Inception_layer1_con_1,Inception_layer1_con_2,MaxPool3])

#Inception Layer 2 
Inception_layer2 = tf.keras.layers.Conv2D(64,(1,1),activation = 'relu',use_bias = 1,kernel_regularizer = 'l2',strides = (1,1),padding ='same')(Concatenate1)
Inception_layer2_con_1 = tf.keras.layers.Conv2D(42,(3,3),activation = 'relu',use_bias = 1,kernel_regularizer = 'l2',strides = (1,1),padding ='same')(Inception_layer2)
Inception_layer2_con_2 = tf.keras.layers.Conv2D(42,(5,5),activation = 'relu',use_bias = 1,kernel_regularizer = 'l2',strides = (1,1),padding ='same')(Inception_layer2)
MaxPool4 = tf.keras.layers.MaxPool2D(pool_size=(3, 3), strides=(1,1),padding = 'same')(Inception_layer2)
Concatenate2 = tf.keras.layers.Concatenate(axis=-1)([Inception_layer2_con_1,Inception_layer2_con_2,MaxPool4])
Concatenate3 =  tf.keras.layers.Concatenate(axis=-1)([Concatenate1, Concatenate2])
layer3 = tf.keras.layers.Conv2D(64,(1,1),activation = 'relu',use_bias = 1,kernel_regularizer = 'l2',strides = (1,1),padding ='same')(Concatenate3)

#Fully Connected Layers
Flatten =  tf.keras.layers.Flatten()(layer3)
hidden_1 = tf.keras.layers.Dense(20, activation = 'relu')(Flatten)
dropout = tf.keras.layers.Dropout(rate = 0.1)(hidden_1)
outputs = tf.keras.layers.Dense(3, activation = tf.keras.activations.softmax)(dropout)

model = tf.keras.Model(inputs = inputs, outputs = outputs)
model.summary()

#Define learning rate and metrics for evaluation of model
lr = 0.0001
METRICS = [
        'accuracy',
        tf.keras.metrics.Precision(name='precision'),
        tf.keras.metrics.Recall(name='recall')
    ]

#Compile model
model.compile(loss='CategoricalCrossentropy', optimizer=Adam(learning_rate=lr), metrics= METRICS)

#Create callbacks
reduce_lr = ReduceLROnPlateau(monitor='val_accuracy', factor=0.8,
                              patience=3, min_lr=0.0000001)
early_stopping = EarlyStopping(monitor='val_loss', patience=5)
bst_model_path = '.h5'
model_checkpoint = ModelCheckpoint(bst_model_path, save_best_only=True, save_weights_only=True)

#Create class weight
weights = compute_class_weight(
                               'balanced', 
                               classes=np.unique(train_generator.classes), 
                               y=train_generator.classes
                               )
cw = dict(zip(np.unique(train_generator.classes), weights))
print(cw)

#Define number of epochs and batch size
max_epochs = 30
batch_size = 32

#Fit the model
history=model.fit(train_generator,
          validation_data=valid_generator,
          epochs=max_epochs,
          verbose=1,
          shuffle=True,
          class_weight=cw,
          callbacks=[reduce_lr, early_stopping, model_checkpoint])

#Make plots

#  "Accuracy"
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'validation'], loc='upper left')
plt.show()
# "Loss"
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'validation'], loc='upper left')
plt.show()
# "Precision"
plt.plot(history.history['precision'])
plt.plot(history.history['val_precision'])
plt.title('model precision')
plt.ylabel('precision')
plt.xlabel('epoch')
plt.legend(['train', 'validation'], loc='upper left')
plt.show()
# "Recall"
plt.plot(history.history['recall'])
plt.plot(history.history['val_recall'])
plt.title('model recall')
plt.ylabel('recall')
plt.xlabel('epoch')
plt.legend(['train', 'validation'], loc='upper left')
plt.show()

#Load best model
model.load_weights(bst_model_path)

#Import test set and make predictions
file_name = []
class_id=[]

for i in glob.glob(r'test_images/*.jpg'):
  im = Image.open('/content/' + i)
  im = im.convert('RGB')
  im = im.resize((224, 224))
  im = np.array(im)
  im = np.expand_dims(im, axis=0)
  im = tf.keras.applications.vgg16.preprocess_input(im)
  prediction = model.predict(im)
  #print(prediction)
  label = np.argmax(prediction, axis=1)
  #print(label)
  file_name.append(i[12:39])
  class_id.append(label[0])

#Create a dataframe for name of images and their class prediction
df = pd.DataFrame(list(zip(file_name, class_id)),
                  columns = ['file_name', 'class_id'])

#Convert dataframe to csv in order to upload to Kaggle
df.to_csv('/content/results.csv', index = False)
